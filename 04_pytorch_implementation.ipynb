{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b545b5",
   "metadata": {},
   "source": [
    "# Neural Networks Practical Workshop - Part 4: PyTorch Implementation\n",
    "\n",
    "In this notebook, we'll use PyTorch to build and train a neural network for MNIST digit recognition. PyTorch provides a powerful framework for developing deep learning models with automatic differentiation, GPU acceleration, and many built-in tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b646935",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2e2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f8086",
   "metadata": {},
   "source": [
    "## 2. Load MNIST Dataset\n",
    "\n",
    "We'll either use the full MNIST dataset loaded via torchvision, or load our preprocessed subsets from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load preprocessed datasets\n",
    "try:\n",
    "    # Load the preprocessed dataset from the previous notebook\n",
    "    data = np.load('./processed_data/mnist_subset.npz')\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(\"Loaded preprocessed MNIST subset from file\")\n",
    "    print(f\"Training set: {len(train_dataset)} samples\")\n",
    "    print(f\"Test set: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Flag to indicate data format\n",
    "    data_format = \"flat\"  # Data is already flattened\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    # Option 2: If file not found, load the full dataset from torchvision\n",
    "    print(\"Preprocessed data not found. Loading full MNIST dataset...\")\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Download and load the MNIST dataset\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Training set: {len(train_dataset)} samples\")\n",
    "    print(f\"Test set: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Flag to indicate data format\n",
    "    data_format = \"image\"  # Data is in image format [1, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1b11e",
   "metadata": {},
   "source": [
    "## 3. Define Neural Network Architecture\n",
    "\n",
    "Let's create a simple neural network architecture for MNIST classification using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13aaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Add a flattening layer if needed\n",
    "        self.flatten = nn.Flatten() if data_format == \"image\" else None\n",
    "        \n",
    "        # Define the network architecture\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input if needed\n",
    "        if self.flatten is not None:\n",
    "            x = self.flatten(x)\n",
    "            \n",
    "        # Pass through the layers\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c054b9",
   "metadata": {},
   "source": [
    "## 4. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f53db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Alternatively, try Adam optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199508c5",
   "metadata": {},
   "source": [
    "## 5. Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss_val = loss.item()\n",
    "            current = batch * len(X)\n",
    "            print(f\"Loss: {loss_val:.4f} [{current}/{len(dataloader.dataset)}]\")\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def test(dataloader, model, loss_fn, device):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    test_loss /= len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:.2f}%, Avg loss: {test_loss:.4f}\")\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3fbb59",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5285016",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "training_time = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\\n-------------------------------\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train(train_loader, model, loss_fn, optimizer, device)\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = test(test_loader, model, loss_fn, device)\n",
    "    \n",
    "    # Record metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Track time\n",
    "    epoch_time = time.time() - start_time\n",
    "    training_time += epoch_time\n",
    "    print(f\"Epoch completed in {epoch_time:.2f} seconds\")\n",
    "    \n",
    "print(f\"Done! Total training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b6916",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe221e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and testing curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25468cbe",
   "metadata": {},
   "source": [
    "## 8. Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "test_examples = iter(test_loader)\n",
    "example_images, example_labels = next(test_examples)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    example_images_device = example_images.to(device)\n",
    "    predictions = model(example_images_device)\n",
    "    predicted_classes = torch.max(predictions, 1)[1]\n",
    "\n",
    "# Display predictions\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(min(9, len(example_images))):\n",
    "    # Check if data is already flattened\n",
    "    if data_format == \"flat\":\n",
    "        img_display = example_images[i].reshape(28, 28).cpu().numpy()\n",
    "    else:\n",
    "        img_display = example_images[i][0].cpu().numpy()\n",
    "        \n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(img_display, cmap='gray')\n",
    "    plt.title(f'Actual: {example_labels[i]}\\nPredicted: {predicted_classes[i].cpu().numpy()}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6de02af",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0be63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions for test set\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "            xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6965b",
   "metadata": {},
   "source": [
    "## 10. Convolutional Neural Network (CNN)\n",
    "\n",
    "Let's also implement a Convolutional Neural Network and compare its performance with our basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad82d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # If input is flattened, reshape it to image format\n",
    "        if data_format == \"flat\":\n",
    "            x = x.view(-1, 1, 28, 28)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the CNN model\n",
    "cnn_model = ConvNet().to(device)\n",
    "print(cnn_model)\n",
    "\n",
    "# Define loss function and optimizer for the CNN\n",
    "cnn_loss_fn = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c9368",
   "metadata": {},
   "source": [
    "## 11. Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbfc040",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_epochs = 5\n",
    "cnn_train_losses = []\n",
    "cnn_train_accuracies = []\n",
    "cnn_test_losses = []\n",
    "cnn_test_accuracies = []\n",
    "cnn_training_time = 0\n",
    "\n",
    "for epoch in range(cnn_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{cnn_epochs}\\n-------------------------------\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train(train_loader, cnn_model, cnn_loss_fn, cnn_optimizer, device)\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = test(test_loader, cnn_model, cnn_loss_fn, device)\n",
    "    \n",
    "    # Record metrics\n",
    "    cnn_train_losses.append(train_loss)\n",
    "    cnn_train_accuracies.append(train_acc)\n",
    "    cnn_test_losses.append(test_loss)\n",
    "    cnn_test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Track time\n",
    "    epoch_time = time.time() - start_time\n",
    "    cnn_training_time += epoch_time\n",
    "    print(f\"Epoch completed in {epoch_time:.2f} seconds\")\n",
    "    \n",
    "print(f\"Done! Total CNN training time: {cnn_training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67184bc",
   "metadata": {},
   "source": [
    "## 12. Compare CNN vs. Basic NN Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance of both models\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, 'b-', label='Basic NN')\n",
    "plt.plot(range(1, cnn_epochs+1), cnn_train_losses, 'r-', label='CNN')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot test loss\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(1, epochs+1), test_losses, 'b-', label='Basic NN')\n",
    "plt.plot(range(1, cnn_epochs+1), cnn_test_losses, 'r-', label='CNN')\n",
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(range(1, epochs+1), train_accuracies, 'b-', label='Basic NN')\n",
    "plt.plot(range(1, cnn_epochs+1), cnn_train_accuracies, 'r-', label='CNN')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(range(1, epochs+1), test_accuracies, 'b-', label='Basic NN')\n",
    "plt.plot(range(1, cnn_epochs+1), cnn_test_accuracies, 'r-', label='CNN')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final test accuracies\n",
    "print(f\"Basic Neural Network - Final test accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Convolutional Neural Network - Final test accuracy: {cnn_test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Training time - Basic NN: {training_time:.2f}s, CNN: {cnn_training_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f523482",
   "metadata": {},
   "source": [
    "## 13. Visualize CNN Activations\n",
    "\n",
    "Let's visualize the activations of the convolutional layers to understand what the CNN is \"seeing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hooks to capture activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "cnn_model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "cnn_model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "\n",
    "# Get a sample image\n",
    "sample_batch = next(iter(test_loader))\n",
    "sample_image, sample_label = sample_batch\n",
    "sample_image_single = sample_image[0:1].to(device)  # Just use the first image\n",
    "\n",
    "# If data is flattened, reshape it\n",
    "if data_format == \"flat\":\n",
    "    display_image = sample_image[0].reshape(28, 28).cpu().numpy()\n",
    "else:\n",
    "    display_image = sample_image[0][0].cpu().numpy()\n",
    "\n",
    "# Forward pass to get activations\n",
    "with torch.no_grad():\n",
    "    output = cnn_model(sample_image_single)\n",
    "\n",
    "# Display the image and its activations\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Display input image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(display_image, cmap='gray')\n",
    "plt.title(f'Input Image (Digit: {sample_label[0]}')\n",
    "plt.axis('off')\n",
    "\n",
    "# Display conv1 activations (first 16 filters)\n",
    "plt.subplot(1, 3, 2)\n",
    "act1 = activations['conv1'][0, :16].cpu().numpy()\n",
    "grid_size = int(np.ceil(np.sqrt(16)))\n",
    "fig_act1 = plt.figure(figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    plt.subplot(grid_size, grid_size, i+1)\n",
    "    plt.imshow(act1[i], cmap='viridis')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('First Conv Layer Activations', y=0.95)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display conv2 activations (first 16 filters)\n",
    "plt.figure(figsize=(10, 10))\n",
    "act2 = activations['conv2'][0, :16].cpu().numpy()\n",
    "for i in range(16):\n",
    "    plt.subplot(grid_size, grid_size, i+1)\n",
    "    plt.imshow(act2[i], cmap='viridis')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Second Conv Layer Activations', y=0.95)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d9528",
   "metadata": {},
   "source": [
    "## 14. Save the Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d895ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save models\n",
    "import os\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "\n",
    "# Save models\n",
    "torch.save(model.state_dict(), './models/basic_nn_mnist.pth')\n",
    "torch.save(cnn_model.state_dict(), './models/cnn_mnist.pth')\n",
    "\n",
    "print(\"Models saved to the 'models' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14f4f8",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we've implemented and trained neural networks using PyTorch, let's move on to the next notebook, `05_comparison.ipynb`, where we'll directly compare our custom implementation with the PyTorch implementation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
