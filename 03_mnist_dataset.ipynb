{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd527384",
   "metadata": {},
   "source": [
    "# Neural Networks Practical Workshop - Part 3: MNIST Dataset\n",
    "\n",
    "In this notebook, we'll explore the MNIST dataset, which is a widely-used collection of handwritten digits. It's often used as a benchmark for image classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a2a16",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c3ef3",
   "metadata": {},
   "source": [
    "## 2. Loading the MNIST Dataset\n",
    "\n",
    "We'll use PyTorch's torchvision library to download and load the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaeab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721cf467",
   "metadata": {},
   "source": [
    "## 3. Exploring the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d834c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "examples = iter(train_loader)\n",
    "images, labels = next(examples)\n",
    "\n",
    "# Print shape information\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Sample image shape: {images[0].shape}\")\n",
    "\n",
    "# Display a few examples\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(images[i][0], cmap='gray')\n",
    "    plt.title(f'Label: {labels[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bca1b8",
   "metadata": {},
   "source": [
    "## 4. Distribution of Digits in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count digit occurrences in training set\n",
    "digit_counts = np.zeros(10)\n",
    "for _, labels in train_loader:\n",
    "    for label in labels:\n",
    "        digit_counts[label.item()] += 1\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), digit_counts)\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Digits in MNIST Training Set')\n",
    "plt.xticks(range(10))\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Digit distribution in the training set:\")\n",
    "for digit in range(10):\n",
    "    print(f\"Digit {digit}: {int(digit_counts[digit])} images ({digit_counts[digit]/sum(digit_counts)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbd509",
   "metadata": {},
   "source": [
    "## 5. Visualizing Image Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average image for each digit\n",
    "avg_images = np.zeros((10, 28, 28))\n",
    "counts = np.zeros(10)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    for image, label in zip(images, labels):\n",
    "        avg_images[label.item()] += image[0].numpy()\n",
    "        counts[label.item()] += 1\n",
    "\n",
    "# Normalize by count\n",
    "for digit in range(10):\n",
    "    avg_images[digit] /= counts[digit]\n",
    "\n",
    "# Display average images\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(avg_images[i], cmap='gray')\n",
    "    plt.title(f'Digit {i}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Average Image for Each Digit')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0272921",
   "metadata": {},
   "source": [
    "## 6. Preparing Data for Custom Neural Network\n",
    "\n",
    "To use our custom neural network implementation from the previous notebook, we need to flatten the images and prepare them in a format compatible with our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a subset of MNIST for faster training with our custom model\n",
    "def get_mnist_subset(loader, num_samples):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    samples_count = 0\n",
    "    \n",
    "    for images, batch_labels in loader:\n",
    "        # Flatten the images\n",
    "        batch_size = images.size(0)\n",
    "        flattened_images = images.view(batch_size, -1).numpy()\n",
    "        batch_labels_np = batch_labels.numpy()\n",
    "        \n",
    "        # Add to our collection\n",
    "        samples.append(flattened_images)\n",
    "        labels.append(batch_labels_np)\n",
    "        \n",
    "        # Update count\n",
    "        samples_count += batch_size\n",
    "        if samples_count >= num_samples:\n",
    "            break\n",
    "    \n",
    "    # Combine all batches\n",
    "    X = np.vstack(samples)[:num_samples]\n",
    "    y = np.concatenate(labels)[:num_samples]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Get a subset of 5000 training examples and 1000 test examples\n",
    "X_train_subset, y_train_subset = get_mnist_subset(train_loader, 5000)\n",
    "X_test_subset, y_test_subset = get_mnist_subset(test_loader, 1000)\n",
    "\n",
    "print(f\"Training subset shape: {X_train_subset.shape}\")\n",
    "print(f\"Training labels shape: {y_train_subset.shape}\")\n",
    "print(f\"Test subset shape: {X_test_subset.shape}\")\n",
    "print(f\"Test labels shape: {y_test_subset.shape}\")\n",
    "\n",
    "# Display a sample image from the flattened data\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(X_train_subset[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f'Label: {y_train_subset[0]}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa930d4",
   "metadata": {},
   "source": [
    "## 7. Data Augmentation\n",
    "\n",
    "Let's explore how data augmentation can be used to artificially expand the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation transformations\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Create a dataset with augmentations\n",
    "train_dataset_aug = datasets.MNIST('./data', train=True, download=True, transform=augmentation_transform)\n",
    "\n",
    "# Create a loader to visualize the augmented data\n",
    "train_loader_aug = DataLoader(train_dataset_aug, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Get a batch of original and augmented data\n",
    "orig_examples = iter(train_loader)\n",
    "aug_examples = iter(train_loader_aug)\n",
    "orig_images, orig_labels = next(orig_examples)\n",
    "aug_images, aug_labels = next(aug_examples)\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    # Original images\n",
    "    axes[0, i].imshow(orig_images[i][0], cmap='gray')\n",
    "    axes[0, i].set_title(f'Original: {orig_labels[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Augmented images\n",
    "    axes[1, i].imshow(aug_images[i][0], cmap='gray')\n",
    "    axes[1, i].set_title(f'Augmented: {aug_labels[i]}')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Original vs Augmented MNIST Images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067f8fba",
   "metadata": {},
   "source": [
    "## 8. Creating a Balanced Mini-Dataset for Experimentation\n",
    "\n",
    "For experimentation and fast prototyping, let's create a small, balanced dataset with an equal number of images per digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833297d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a balanced mini-dataset\n",
    "def create_balanced_mini_dataset(loader, samples_per_class=50):\n",
    "    X = []\n",
    "    y = []\n",
    "    counts = np.zeros(10, dtype=int)\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        batch_size = images.size(0)\n",
    "        flattened_images = images.view(batch_size, -1).numpy()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i].item()\n",
    "            if counts[label] < samples_per_class:\n",
    "                X.append(flattened_images[i])\n",
    "                y.append(label)\n",
    "                counts[label] += 1\n",
    "        \n",
    "        if np.min(counts) >= samples_per_class:\n",
    "            break\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create balanced mini-datasets for training and testing\n",
    "X_mini_train, y_mini_train = create_balanced_mini_dataset(train_loader, samples_per_class=50)\n",
    "X_mini_test, y_mini_test = create_balanced_mini_dataset(test_loader, samples_per_class=20)\n",
    "\n",
    "print(f\"Mini train set shape: {X_mini_train.shape}\")\n",
    "print(f\"Mini test set shape: {X_mini_test.shape}\")\n",
    "\n",
    "# Visualize the distribution\n",
    "unique, counts = np.unique(y_mini_train, return_counts=True)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(unique, counts)\n",
    "plt.title('Distribution of Digits in Balanced Mini-Dataset')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffec6a",
   "metadata": {},
   "source": [
    "## 9. Save the Prepared Data\n",
    "\n",
    "Let's save our prepared datasets so they can be easily loaded in the subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f5c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save our data\n",
    "import os\n",
    "if not os.path.exists('./processed_data'):\n",
    "    os.makedirs('./processed_data')\n",
    "\n",
    "# Save our prepared datasets\n",
    "np.savez('./processed_data/mnist_subset.npz', \n",
    "         X_train=X_train_subset, y_train=y_train_subset,\n",
    "         X_test=X_test_subset, y_test=y_test_subset)\n",
    "\n",
    "np.savez('./processed_data/mnist_mini.npz',\n",
    "         X_train=X_mini_train, y_train=y_mini_train,\n",
    "         X_test=X_mini_test, y_test=y_mini_test)\n",
    "\n",
    "print(\"Datasets saved to 'processed_data' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbde3b6",
   "metadata": {},
   "source": [
    "## 10. Visualization of Image Features\n",
    "\n",
    "Let's visualize some features of the MNIST images to better understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bcf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate pixel intensity distribution\n",
    "pixel_values = []\n",
    "for images, _ in train_loader:\n",
    "    # Add pixel values from this batch\n",
    "    pixel_values.extend(images.numpy().flatten())\n",
    "    # Limit the number of batches to process\n",
    "    if len(pixel_values) > 100000:  # sample about 100k pixels\n",
    "        break\n",
    "pixel_values = np.array(pixel_values)\n",
    "\n",
    "# Plot histogram of pixel intensities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pixel_values, bins=50)\n",
    "plt.title('Distribution of Pixel Intensities in MNIST')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d0b14",
   "metadata": {},
   "source": [
    "## 11. Visualize Difficult Examples\n",
    "\n",
    "Let's find and visualize some examples that might be difficult for a model to classify correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find images with minimal black pixels (potentially harder to classify)\n",
    "def find_difficult_examples(loader, num_examples=10):\n",
    "    difficult_images = []\n",
    "    difficult_labels = []\n",
    "    pixel_counts = []  # Lower count = fewer dark pixels = potentially harder\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        for i in range(images.size(0)):\n",
    "            # Count darker pixels (below a threshold)\n",
    "            img = images[i][0].numpy()\n",
    "            # Calculate the number of pixels with intensity < -0.5 after normalization\n",
    "            dark_pixel_count = np.sum(img < -0.5)\n",
    "            \n",
    "            # If we don't have enough examples yet or this one has fewer dark pixels\n",
    "            if len(difficult_images) < num_examples:\n",
    "                difficult_images.append(img)\n",
    "                difficult_labels.append(labels[i].item())\n",
    "                pixel_counts.append(dark_pixel_count)\n",
    "            elif dark_pixel_count < max(pixel_counts):\n",
    "                # Replace the example with the highest pixel count\n",
    "                max_idx = pixel_counts.index(max(pixel_counts))\n",
    "                difficult_images[max_idx] = img\n",
    "                difficult_labels[max_idx] = labels[i].item()\n",
    "                pixel_counts[max_idx] = dark_pixel_count\n",
    "    \n",
    "    return difficult_images, difficult_labels\n",
    "\n",
    "# Find potentially difficult examples\n",
    "difficult_images, difficult_labels = find_difficult_examples(test_loader)\n",
    "\n",
    "# Display the difficult examples\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(len(difficult_images)):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(difficult_images[i], cmap='gray')\n",
    "    plt.title(f'Label: {difficult_labels[i]}')\n",
    "    plt.axis('off')\n",
    "plt.suptitle('Potentially Difficult MNIST Examples')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8e3dd",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we've explored the MNIST dataset and prepared it for use with our models, let's move on to the next notebook, `04_pytorch_implementation.ipynb`, where we'll implement a neural network using PyTorch to recognize handwritten digits."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
