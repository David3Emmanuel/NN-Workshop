{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255f0d75",
   "metadata": {},
   "source": [
    "# Neural Networks Practical Workshop\n",
    "\n",
    "In this workshop, we'll explore neural networks from the ground up. We'll first implement a neural network from scratch using NumPy, then use PyTorch for more efficient implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f4aae",
   "metadata": {},
   "source": [
    "## 1. Setup and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3200b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d10ef1",
   "metadata": {},
   "source": [
    "## 2. Neural Network from Scratch using NumPy\n",
    "\n",
    "Let's start by implementing a simple neural network from scratch to understand the fundamental concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234c845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkFromScratch:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Derivative of sigmoid for backpropagation\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # Softmax activation for output layer\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        # Backpropagation\n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.zeros((y.size, output.shape[1]))\n",
    "        y_one_hot[np.arange(y.size), y] = 1\n",
    "        \n",
    "        # Calculate gradients\n",
    "        dz2 = output - y_one_hot\n",
    "        dW2 = np.dot(self.a1.T, dz2)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.sigmoid_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, dz1)\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.1, epochs=1000):\n",
    "        # Training loop\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            y_one_hot = np.zeros((y.size, output.shape[1]))\n",
    "            y_one_hot[np.arange(y.size), y] = 1\n",
    "            loss = -np.sum(y_one_hot * np.log(output + 1e-8)) / y.size\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9614a",
   "metadata": {},
   "source": [
    "### 2.1 Test our Neural Network from Scratch with a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b27c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset (XOR problem)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize and train the network\n",
    "nn_scratch = NeuralNetworkFromScratch(input_size=2, hidden_size=4, output_size=2)\n",
    "losses = nn_scratch.train(X, y, learning_rate=0.5, epochs=5000)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over training epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Test the network\n",
    "predictions = nn_scratch.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual:     \", y)\n",
    "print(f\"Accuracy: {np.mean(predictions == y) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb64c9",
   "metadata": {},
   "source": [
    "## 3. Load the MNIST Dataset for a Real-World Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the MNIST dataset\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a20060",
   "metadata": {},
   "source": [
    "### 3.1 Visualize Some MNIST Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples\n",
    "examples = iter(train_loader)\n",
    "images, labels = next(examples)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(images[i][0], cmap='gray')\n",
    "    plt.title(f'Label: {labels[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b76db1",
   "metadata": {},
   "source": [
    "## 4. Neural Network with PyTorch\n",
    "\n",
    "Now that we understand the fundamentals, let's use PyTorch to build a more efficient neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34080a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517d486",
   "metadata": {},
   "source": [
    "### 4.1 Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed05a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch [{batch}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def test(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    test_loss /= len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Error: \\n Accuracy: {accuracy:.2f}%, Avg loss: {test_loss:.4f}\")\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4324f37",
   "metadata": {},
   "source": [
    "### 4.2 Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\\n-------------------------------\")\n",
    "    train_loss, train_acc = train(model, train_loader, loss_fn, optimizer, device)\n",
    "    test_loss, test_acc = test(model, test_loader, loss_fn, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff98a1",
   "metadata": {},
   "source": [
    "### 4.3 Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs+1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bccb27",
   "metadata": {},
   "source": [
    "### 4.4 Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fd80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "test_examples = iter(test_loader)\n",
    "test_images, test_labels = next(test_examples)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    test_images_device = test_images.to(device)\n",
    "    predictions = model(test_images_device)\n",
    "    predicted_classes = torch.max(predictions, 1)[1]\n",
    "\n",
    "# Display predictions\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(test_images[i][0], cmap='gray')\n",
    "    plt.title(f'Actual: {test_labels[i]}, Predicted: {predicted_classes[i].cpu().numpy()}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b2f75",
   "metadata": {},
   "source": [
    "## 5. Experimentation\n",
    "\n",
    "Now it's your turn to experiment. Try changing these aspects of the model:\n",
    "\n",
    "1. Number of hidden layers and neurons\n",
    "2. Activation functions (ReLU, Tanh, Sigmoid, etc.)\n",
    "3. Learning rate and optimizer\n",
    "4. Batch size\n",
    "5. Regularization techniques (Dropout, L2 regularization)\n",
    "\n",
    "Note how these changes affect the model's performance and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454354ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Create a model with dropout for regularization\n",
    "class NeuralNetworkWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(NeuralNetworkWithDropout, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model with dropout\n",
    "model_dropout = NeuralNetworkWithDropout(dropout_rate=0.3).to(device)\n",
    "\n",
    "# Change optimizer to Adam with a different learning rate\n",
    "optimizer_dropout = optim.Adam(model_dropout.parameters(), lr=0.001)\n",
    "\n",
    "# You can now train this new model using the same training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71bac5c",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusions\n",
    "\n",
    "In this workshop, we've:\n",
    "\n",
    "1. Implemented a neural network from scratch to understand the fundamentals\n",
    "2. Used PyTorch to build and train a more efficient model\n",
    "3. Visualized the model's performance and predictions\n",
    "4. Experimented with different model architectures and hyperparameters\n",
    "\n",
    "Key takeaways:\n",
    "- The fundamental operations (forward pass, activation, backpropagation) are the same in both implementations\n",
    "- PyTorch provides automatic differentiation and optimized operations for faster training\n",
    "- Model architecture and hyperparameters significantly impact performance\n",
    "- Understanding the underlying principles helps in designing and debugging neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b55bb1",
   "metadata": {},
   "source": [
    "## 7. Custom Network vs PyTorch: Direct Comparison\n",
    "\n",
    "Let's directly compare a neural network built from scratch with a PyTorch model using identical architectures on the MNIST dataset. This will help us understand the differences in implementation, performance, and training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare MNIST data for the custom model (flattened format)\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get a smaller subset of MNIST for faster training\n",
    "train_subset_size = 5000\n",
    "test_subset_size = 1000\n",
    "\n",
    "# Extract a subset of training data\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    batch_size = len(labels)\n",
    "    if len(train_labels) + batch_size > train_subset_size:\n",
    "        # Take only what we need to reach the subset size\n",
    "        remaining = train_subset_size - len(train_labels)\n",
    "        train_samples.append(images[:remaining].view(remaining, -1).cpu().numpy())\n",
    "        train_labels.append(labels[:remaining].cpu().numpy())\n",
    "        break\n",
    "    else:\n",
    "        train_samples.append(images.view(batch_size, -1).cpu().numpy())\n",
    "        train_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Extract a subset of test data\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    batch_size = len(labels)\n",
    "    if len(test_labels) + batch_size > test_subset_size:\n",
    "        # Take only what we need to reach the subset size\n",
    "        remaining = test_subset_size - len(test_labels)\n",
    "        test_samples.append(images[:remaining].view(remaining, -1).cpu().numpy())\n",
    "        test_labels.append(labels[:remaining].cpu().numpy())\n",
    "        break\n",
    "    else:\n",
    "        test_samples.append(images.view(batch_size, -1).cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists of arrays to single arrays\n",
    "X_train = np.vstack(train_samples)\n",
    "y_train = np.concatenate(train_labels)\n",
    "X_test = np.vstack(test_samples)\n",
    "y_test = np.concatenate(test_labels)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85fa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network with the same architecture as our PyTorch model\n",
    "class MNISTNeuralNetworkFromScratch:\n",
    "    def __init__(self, input_size=784, hidden1_size=128, hidden2_size=64, output_size=10):\n",
    "        # Initialize weights and biases with Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * np.sqrt(1 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(1 / hidden1_size)\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        self.W3 = np.random.randn(hidden2_size, output_size) * np.sqrt(1 / hidden2_size)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        # ReLU activation function\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        # Derivative of ReLU for backpropagation\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # Softmax activation for output layer\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = self.softmax(self.z3)\n",
    "        return self.a3\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate=0.01):\n",
    "        # Backpropagation\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.zeros((y.size, output.shape[1]))\n",
    "        y_one_hot[np.arange(y.size), y] = 1\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz3 = output - y_one_hot\n",
    "        dW3 = (1/batch_size) * np.dot(self.a2.T, dz3)\n",
    "        db3 = (1/batch_size) * np.sum(dz3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Second hidden layer gradients\n",
    "        da2 = np.dot(dz3, self.W3.T)\n",
    "        dz2 = da2 * self.relu_derivative(self.a2)\n",
    "        dW2 = (1/batch_size) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/batch_size) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # First hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.relu_derivative(self.a1)\n",
    "        dW1 = (1/batch_size) * np.dot(X.T, dz1)\n",
    "        db1 = (1/batch_size) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update parameters with gradient descent\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Cross-entropy loss\n",
    "        # Convert y_true to one-hot encoding\n",
    "        y_one_hot = np.zeros((y_true.size, y_pred.shape[1]))\n",
    "        y_one_hot[np.arange(y_true.size), y_true] = 1\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        loss = -np.mean(np.sum(y_one_hot * np.log(y_pred + 1e-8), axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, batch_size=64, epochs=10, learning_rate=0.01):\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        # Keep track of metrics\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_correct = 0\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                # Get batch\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, n_samples)\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "                \n",
    "                # Compute loss and accuracy\n",
    "                batch_loss = self.compute_loss(y_batch, y_pred)\n",
    "                epoch_loss += batch_loss * (end_idx - start_idx)\n",
    "                \n",
    "                # Count correct predictions\n",
    "                batch_preds = np.argmax(y_pred, axis=1)\n",
    "                epoch_correct += np.sum(batch_preds == y_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(X_batch, y_batch, y_pred, learning_rate)\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            epoch_loss /= n_samples\n",
    "            epoch_accuracy = epoch_correct / n_samples * 100\n",
    "            \n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        return losses, accuracies, training_time\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an equivalent PyTorch model with exactly the same architecture\n",
    "class EquivalentPyTorchModel(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden1_size=128, hidden2_size=64, output_size=10):\n",
    "        super(EquivalentPyTorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        # Not including softmax here as CrossEntropyLoss includes it\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the PyTorch model\n",
    "pytorch_model = EquivalentPyTorchModel().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "py_optimizer = optim.SGD(pytorch_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Create data loaders for PyTorch training\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8241f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the PyTorch model\n",
    "def train_pytorch_model(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_accuracy = correct / total * 100\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return losses, accuracies, training_time\n",
    "\n",
    "# Function to evaluate the PyTorch model\n",
    "def evaluate_pytorch_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30484bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the custom neural network\n",
    "print(\"Training custom neural network from scratch...\")\n",
    "custom_nn = MNISTNeuralNetworkFromScratch(input_size=784, hidden1_size=128, hidden2_size=64, output_size=10)\n",
    "custom_losses, custom_accuracies, custom_time = custom_nn.train(\n",
    "    X_train, y_train, batch_size=64, epochs=10, learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Evaluate custom model on test data\n",
    "custom_predictions = custom_nn.predict(X_test)\n",
    "custom_test_accuracy = np.mean(custom_predictions == y_test) * 100\n",
    "print(f\"Custom model test accuracy: {custom_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ada36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PyTorch model\n",
    "print(\"\\nTraining PyTorch model...\")\n",
    "pytorch_losses, pytorch_accuracies, pytorch_time = train_pytorch_model(\n",
    "    pytorch_model, train_dataloader, criterion, py_optimizer, device, num_epochs=10\n",
    ")\n",
    "\n",
    "# Evaluate PyTorch model on test data\n",
    "pytorch_test_accuracy, pytorch_predictions, test_labels = evaluate_pytorch_model(\n",
    "    pytorch_model, test_dataloader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc10226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"{'Model':<20} {'Training Time (s)':<20} {'Test Accuracy (%)':<20}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Custom Neural Net':<20} {custom_time:<20.2f} {custom_test_accuracy:<20.2f}\")\n",
    "print(f\"{'PyTorch Model':<20} {pytorch_time:<20.2f} {pytorch_test_accuracy:<20.2f}\")\n",
    "print(f\"{'Speedup':<20} {custom_time/pytorch_time:.2f}x\")\n",
    "\n",
    "# Plot training metrics comparison\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(1, len(custom_losses) + 1), custom_losses, 'b-', label='Custom Implementation')\n",
    "plt.plot(range(1, len(pytorch_losses) + 1), pytorch_losses, 'r-', label='PyTorch Implementation')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(1, len(custom_accuracies) + 1), custom_accuracies, 'b-', label='Custom Implementation')\n",
    "plt.plot(range(1, len(pytorch_accuracies) + 1), pytorch_accuracies, 'r-', label='PyTorch Implementation')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9be6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions visually\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Select a few random test examples\n",
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(2, num_samples, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "    plt.bar(['Custom', 'PyTorch'], [custom_predictions[idx], pytorch_predictions[idx]], color=['blue', 'red'])\n",
    "    plt.title(f\"Custom: {custom_predictions[idx]}, PyTorch: {pytorch_predictions[idx]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b15cac",
   "metadata": {},
   "source": [
    "### Discussion of Comparison Results\n",
    "\n",
    "From the experiment above, we can observe several key points:\n",
    "\n",
    "1. **Training Speed**: PyTorch typically trains much faster due to its optimized backend, GPU acceleration, and vectorized operations.\n",
    "\n",
    "2. **Implementation Complexity**: Our custom implementation required explicit coding of the forward and backward passes, while PyTorch handled this automatically through its autograd system.\n",
    "\n",
    "3. **Performance**: Both implementations should theoretically converge to similar results given enough training time, but PyTorch's optimized operations often lead to better performance and stability.\n",
    "\n",
    "4. **Memory Usage**: Custom implementations may use less memory for small models but don't scale as efficiently for larger networks.\n",
    "\n",
    "5. **Code Length**: The custom implementation required significantly more code to achieve the same functionality.\n",
    "\n",
    "## Key Insights from Building Neural Networks from Scratch\n",
    "\n",
    "- Understanding the fundamental operations helps debug complex models in modern frameworks\n",
    "- Implementing backpropagation from scratch gives insights into gradient flow and training dynamics\n",
    "- Knowledge of the underlying mathematics makes it easier to adapt and extend existing architectures\n",
    "- Frameworks like PyTorch abstract away much of the complexity while maintaining flexibility for research and application"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
