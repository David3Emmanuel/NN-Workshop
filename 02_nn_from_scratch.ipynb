{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90af2169",
   "metadata": {},
   "source": [
    "# Neural Networks Practical Workshop - Part 2: Building a Neural Network from Scratch\n",
    "\n",
    "In this notebook, we'll implement a simple neural network from scratch using NumPy to understand the fundamental concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52210bff",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5fb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a3a00",
   "metadata": {},
   "source": [
    "## 2. Neural Network Implementation\n",
    "\n",
    "Let's create a simple neural network class from scratch that can handle a basic feed-forward network with one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f13e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkFromScratch:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid activation function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Derivative of sigmoid for backpropagation\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # Softmax activation for output layer\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        # Backpropagation\n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.zeros((y.size, output.shape[1]))\n",
    "        y_one_hot[np.arange(y.size), y] = 1\n",
    "        \n",
    "        # Calculate gradients\n",
    "        dz2 = output - y_one_hot\n",
    "        dW2 = np.dot(self.a1.T, dz2)\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.sigmoid_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, dz1)\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.1, epochs=1000):\n",
    "        # Training loop\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            y_one_hot = np.zeros((y.size, output.shape[1]))\n",
    "            y_one_hot[np.arange(y.size), y] = 1\n",
    "            loss = -np.sum(y_one_hot * np.log(output + 1e-8)) / y.size\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backpropagation\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.W1 -= learning_rate * dW1\n",
    "            self.b1 -= learning_rate * db1\n",
    "            self.W2 -= learning_rate * dW2\n",
    "            self.b2 -= learning_rate * db2\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59227bb1",
   "metadata": {},
   "source": [
    "## 3. Testing on a Toy Dataset\n",
    "\n",
    "Let's test our neural network implementation on the XOR problem, which is a classic example that requires a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a49602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset (XOR problem)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize and train the network\n",
    "nn_scratch = NeuralNetworkFromScratch(input_size=2, hidden_size=4, output_size=2)\n",
    "losses = nn_scratch.train(X, y, learning_rate=0.5, epochs=5000)\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over training epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Test the network\n",
    "predictions = nn_scratch.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual:     \", y)\n",
    "print(f\"Accuracy: {np.mean(predictions == y) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8f5a0",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Decision Boundary\n",
    "\n",
    "Let's create a visualization of the decision boundary learned by our neural network for the XOR problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh grid\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Make predictions on the mesh grid points\n",
    "Z = nn_scratch.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary for XOR Problem')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f00bb0",
   "metadata": {},
   "source": [
    "## 5. Building a Multi-Layer Neural Network from Scratch\n",
    "\n",
    "Now, let's extend our implementation to handle multiple hidden layers. This implementation will be more flexible and can be used for more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNeuralNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"Initialize a neural network with multiple layers.\n",
    "        \n",
    "        Args:\n",
    "            layer_sizes: List of integers, representing the size of each layer\n",
    "                         (including input and output layers)\n",
    "        \"\"\"\n",
    "        self.num_layers = len(layer_sizes) - 1\n",
    "        self.layer_sizes = layer_sizes\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Xavier initialization for weights\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * \n",
    "                               np.sqrt(1 / layer_sizes[i]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of ReLU function.\"\"\"\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax activation function.\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation.\"\"\"\n",
    "        self.Z = []\n",
    "        self.A = [X]  # Input as the first activation\n",
    "        \n",
    "        # Hidden layers with ReLU activation\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(self.A[-1], self.weights[i]) + self.biases[i]\n",
    "            a = self.relu(z)\n",
    "            self.Z.append(z)\n",
    "            self.A.append(a)\n",
    "        \n",
    "        # Output layer with softmax activation\n",
    "        z = np.dot(self.A[-1], self.weights[-1]) + self.biases[-1]\n",
    "        a = self.softmax(z)\n",
    "        self.Z.append(z)\n",
    "        self.A.append(a)\n",
    "        \n",
    "        return self.A[-1]\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backward propagation.\"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        gradients = {'dW': [], 'db': []}\n",
    "        \n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.zeros((y.size, output.shape[1]))\n",
    "        y_one_hot[np.arange(y.size), y] = 1\n",
    "        \n",
    "        # Output layer error\n",
    "        dZ = output - y_one_hot\n",
    "        \n",
    "        for layer in range(self.num_layers - 1, -1, -1):\n",
    "            # Calculate gradients for this layer\n",
    "            dW = (1/batch_size) * np.dot(self.A[layer].T, dZ)\n",
    "            db = (1/batch_size) * np.sum(dZ, axis=0, keepdims=True)\n",
    "            \n",
    "            # Store gradients\n",
    "            gradients['dW'].insert(0, dW)\n",
    "            gradients['db'].insert(0, db)\n",
    "            \n",
    "            if layer > 0:\n",
    "                # Compute error for previous layer\n",
    "                dA = np.dot(dZ, self.weights[layer].T)\n",
    "                dZ = dA * self.relu_derivative(self.A[layer])\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.01, epochs=1000, batch_size=32, print_every=100):\n",
    "        \"\"\"Training the neural network.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = max(n_samples // batch_size, 1)\n",
    "        \n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            epoch_correct = 0\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, n_samples)\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                output = self.forward(X_batch)\n",
    "                \n",
    "                # Calculate loss\n",
    "                y_one_hot = np.zeros((y_batch.size, output.shape[1]))\n",
    "                y_one_hot[np.arange(y_batch.size), y_batch] = 1\n",
    "                batch_loss = -np.sum(y_one_hot * np.log(output + 1e-8)) / y_batch.size\n",
    "                epoch_loss += batch_loss * (end_idx - start_idx)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predictions = np.argmax(output, axis=1)\n",
    "                epoch_correct += np.sum(predictions == y_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                gradients = self.backward(X_batch, y_batch, output)\n",
    "                \n",
    "                # Update weights and biases\n",
    "                for layer in range(self.num_layers):\n",
    "                    self.weights[layer] -= learning_rate * gradients['dW'][layer]\n",
    "                    self.biases[layer] -= learning_rate * gradients['db'][layer]\n",
    "            \n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss /= n_samples\n",
    "            epoch_accuracy = epoch_correct / n_samples * 100\n",
    "            \n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_accuracy)\n",
    "            \n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        return losses, accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7a528",
   "metadata": {},
   "source": [
    "## 6. Testing the Multi-Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the multi-layer neural network on the XOR problem\n",
    "layer_sizes = [2, 8, 4, 2]  # Input, hidden layers, output\n",
    "mlnn = MultiLayerNeuralNetwork(layer_sizes)\n",
    "\n",
    "losses, accuracies = mlnn.train(X, y, learning_rate=0.05, epochs=2000, batch_size=4, print_every=200)\n",
    "\n",
    "# Plot the loss and accuracy curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies)\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test predictions\n",
    "predictions = mlnn.predict(X)\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Actual:     \", y)\n",
    "print(f\"Accuracy: {np.mean(predictions == y) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76330068",
   "metadata": {},
   "source": [
    "## 7. Visualizing the Decision Boundary of the Multi-Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh grid\n",
    "h = 0.01\n",
    "x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Make predictions on the mesh grid points\n",
    "Z = mlnn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary for XOR Problem (Multi-Layer Network)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a6956",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have built and tested neural networks from scratch, let's move on to working with a real-world dataset. In the next notebook, `03_mnist_dataset.ipynb`, we'll explore the MNIST dataset for handwritten digit recognition."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
