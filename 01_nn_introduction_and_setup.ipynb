{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786d246e",
   "metadata": {},
   "source": [
    "# Neural Networks Practical Workshop - Part 1: Introduction and Setup\n",
    "\n",
    "In this workshop, we'll explore neural networks from the ground up. We'll first implement a neural network from scratch using NumPy, then use PyTorch for more efficient implementations.\n",
    "\n",
    "## Workshop Overview\n",
    "\n",
    "This workshop is divided into five parts:\n",
    "\n",
    "1. **Introduction and Setup** (this notebook)\n",
    "2. **Neural Network from Scratch**: Building a neural network using only NumPy\n",
    "3. **MNIST Dataset**: Exploring the dataset we'll use for our models\n",
    "4. **PyTorch Implementation**: Using PyTorch to build and train the same model\n",
    "5. **Comparison**: Directly comparing the custom and PyTorch implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc24a35",
   "metadata": {},
   "source": [
    "## 1. Setup and Libraries\n",
    "\n",
    "We'll start by importing the libraries we'll need throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5adfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163ed1c",
   "metadata": {},
   "source": [
    "## 2. Neural Network Fundamentals\n",
    "\n",
    "### What is a Neural Network?\n",
    "\n",
    "A neural network is a series of algorithms that attempts to identify underlying relationships in a set of data through a process that mimics how the human brain operates. They are particularly good at recognizing patterns and can be used for classification, regression, and other tasks.\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Neurons**: The basic units that process information, applying weights to inputs and passing the result through an activation function.\n",
    "2. **Layers**: Collections of neurons that process information in stages.\n",
    "3. **Weights and Biases**: Parameters that are learned during training.\n",
    "4. **Activation Functions**: Non-linear functions that determine the output of a neuron.\n",
    "5. **Loss Function**: Measures how well the network's predictions match the target values.\n",
    "6. **Optimizer**: Algorithm that adjusts the weights and biases to minimize the loss function.\n",
    "\n",
    "### Forward and Backward Propagation\n",
    "\n",
    "- **Forward Propagation**: The process of computing outputs by passing inputs through the network's layers.\n",
    "- **Backward Propagation**: The process of computing gradients and updating weights by propagating errors backward through the network.\n",
    "\n",
    "![Neural Network Architecture](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3f476",
   "metadata": {},
   "source": [
    "## 3. Neural Network Mathematics\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "For a simple feedforward neural network with one hidden layer:\n",
    "\n",
    "1. First layer computation: $z^{[1]} = W^{[1]} \\cdot X + b^{[1]}$\n",
    "2. Apply activation function: $a^{[1]} = g^{[1]}(z^{[1]})$\n",
    "3. Output layer computation: $z^{[2]} = W^{[2]} \\cdot a^{[1]} + b^{[2]}$\n",
    "4. Apply output activation function: $a^{[2]} = g^{[2]}(z^{[2]})$\n",
    "\n",
    "Where:\n",
    "- $X$ is the input\n",
    "- $W^{[l]}$ is the weight matrix for layer $l$\n",
    "- $b^{[l]}$ is the bias vector for layer $l$\n",
    "- $g^{[l]}$ is the activation function for layer $l$\n",
    "- $z^{[l]}$ is the weighted input to layer $l$\n",
    "- $a^{[l]}$ is the activation output of layer $l$\n",
    "\n",
    "### Backward Pass\n",
    "\n",
    "The backward pass involves:\n",
    "\n",
    "1. Compute the output error: $dz^{[2]} = a^{[2]} - y$\n",
    "2. Compute gradients for the output layer: $dW^{[2]} = dz^{[2]} \\cdot a^{[1]T}$, $db^{[2]} = sum(dz^{[2]})$\n",
    "3. Propagate error to hidden layer: $dz^{[1]} = W^{[2]T} \\cdot dz^{[2]} * g^{[1]'}(z^{[1]})$\n",
    "4. Compute gradients for the hidden layer: $dW^{[1]} = dz^{[1]} \\cdot X^T$, $db^{[1]} = sum(dz^{[1]})$\n",
    "5. Update weights: $W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$\n",
    "6. Update biases: $b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$\n",
    "\n",
    "Where:\n",
    "- $y$ is the target output\n",
    "- $dW^{[l]}$ is the gradient of the cost function with respect to $W^{[l]}$\n",
    "- $db^{[l]}$ is the gradient of the cost function with respect to $b^{[l]}$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $g^{[l]'}$ is the derivative of the activation function for layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9f30c",
   "metadata": {},
   "source": [
    "## 4. Common Activation Functions\n",
    "\n",
    "Let's implement and visualize some common activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98873053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Plot activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('Sigmoid')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, relu(x))\n",
    "plt.title('ReLU')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, tanh(x))\n",
    "plt.title('Tanh')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, leaky_relu(x))\n",
    "plt.title('Leaky ReLU')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdedcec2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we understand the basics and have set up our environment, we're ready to implement a neural network from scratch in the next notebook: `02_nn_from_scratch.ipynb`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
