{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "867a9a0e",
   "metadata": {},
   "source": [
    "# Neural Networks Practical Workshop - Part 5: Custom vs. PyTorch Comparison\n",
    "\n",
    "In this notebook, we'll directly compare a neural network built from scratch with an equivalent PyTorch model. Both models will have the same architecture and will be trained and tested on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c2dc3",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688e516",
   "metadata": {},
   "source": [
    "## 2. Load MNIST Data\n",
    "\n",
    "We'll use the preprocessed subset of the MNIST dataset to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bdcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset\n",
    "try:\n",
    "    # Load data from file\n",
    "    data = np.load('./processed_data/mnist_subset.npz')\n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    print(\"Loaded preprocessed MNIST subset:\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Preprocessed data not found. Run the '03_mnist_dataset.ipynb' notebook first.\")\n",
    "    # Dummy data for demonstration\n",
    "    print(\"Creating small dummy dataset for demonstration...\")\n",
    "    X_train = np.random.randn(1000, 784)\n",
    "    y_train = np.random.randint(0, 10, 1000)\n",
    "    X_test = np.random.randn(200, 784)\n",
    "    y_test = np.random.randint(0, 10, 200)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "# Create PyTorch datasets and data loaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3781e3",
   "metadata": {},
   "source": [
    "## 3. Custom Neural Network Implementation\n",
    "\n",
    "Let's create a neural network from scratch with the same architecture as our PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b626e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNeuralNetworkFromScratch:\n",
    "    def __init__(self, input_size=784, hidden1_size=128, hidden2_size=64, output_size=10):\n",
    "        # Initialize weights and biases with Xavier initialization\n",
    "        self.W1 = np.random.randn(input_size, hidden1_size) * np.sqrt(1 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        self.W2 = np.random.randn(hidden1_size, hidden2_size) * np.sqrt(1 / hidden1_size)\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        self.W3 = np.random.randn(hidden2_size, output_size) * np.sqrt(1 / hidden2_size)\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        # ReLU activation function\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        # Derivative of ReLU for backpropagation\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # Softmax activation for output layer\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.relu(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W3) + self.b3\n",
    "        self.a3 = self.softmax(self.z3)\n",
    "        return self.a3\n",
    "    \n",
    "    def backward(self, X, y, output, learning_rate=0.01):\n",
    "        # Backpropagation\n",
    "        batch_size = X.shape[0]\n",
    "        \n",
    "        # Convert y to one-hot encoding\n",
    "        y_one_hot = np.zeros((y.size, output.shape[1]))\n",
    "        y_one_hot[np.arange(y.size), y] = 1\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz3 = output - y_one_hot\n",
    "        dW3 = (1/batch_size) * np.dot(self.a2.T, dz3)\n",
    "        db3 = (1/batch_size) * np.sum(dz3, axis=0, keepdims=True)\n",
    "        \n",
    "        # Second hidden layer gradients\n",
    "        da2 = np.dot(dz3, self.W3.T)\n",
    "        dz2 = da2 * self.relu_derivative(self.a2)\n",
    "        dW2 = (1/batch_size) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/batch_size) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # First hidden layer gradients\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.relu_derivative(self.a1)\n",
    "        dW1 = (1/batch_size) * np.dot(X.T, dz1)\n",
    "        db1 = (1/batch_size) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update parameters with gradient descent\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Cross-entropy loss\n",
    "        # Convert y_true to one-hot encoding\n",
    "        y_one_hot = np.zeros((y_true.size, y_pred.shape[1]))\n",
    "        y_one_hot[np.arange(y_true.size), y_true] = 1\n",
    "        \n",
    "        # Calculate cross-entropy loss\n",
    "        loss = -np.mean(np.sum(y_one_hot * np.log(y_pred + 1e-8), axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, batch_size=64, epochs=10, learning_rate=0.01):\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        \n",
    "        # Keep track of metrics\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_correct = 0\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                # Get batch\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, n_samples)\n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.forward(X_batch)\n",
    "                \n",
    "                # Compute loss and accuracy\n",
    "                batch_loss = self.compute_loss(y_batch, y_pred)\n",
    "                epoch_loss += batch_loss * (end_idx - start_idx)\n",
    "                \n",
    "                # Count correct predictions\n",
    "                batch_preds = np.argmax(y_pred, axis=1)\n",
    "                epoch_correct += np.sum(batch_preds == y_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.backward(X_batch, y_batch, y_pred, learning_rate)\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            epoch_loss /= n_samples\n",
    "            epoch_accuracy = epoch_correct / n_samples * 100\n",
    "            \n",
    "            losses.append(epoch_loss)\n",
    "            accuracies.append(epoch_accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        return losses, accuracies, training_time\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions\n",
    "        output = self.forward(X)\n",
    "        return np.argmax(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8459952",
   "metadata": {},
   "source": [
    "## 4. Equivalent PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c41063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivalentPyTorchModel(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden1_size=128, hidden2_size=64, output_size=10):\n",
    "        super(EquivalentPyTorchModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        # Not including softmax here as CrossEntropyLoss includes it\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the PyTorch model\n",
    "pytorch_model = EquivalentPyTorchModel().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "py_optimizer = optim.SGD(pytorch_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d79ad9",
   "metadata": {},
   "source": [
    "## 5. PyTorch Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d883ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_model(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_accuracy = correct / total * 100\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return losses, accuracies, training_time\n",
    "\n",
    "def evaluate_pytorch_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212e2ac",
   "metadata": {},
   "source": [
    "## 6. Train the Custom Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training custom neural network from scratch...\")\n",
    "custom_nn = MNISTNeuralNetworkFromScratch(\n",
    "    input_size=784, hidden1_size=128, hidden2_size=64, output_size=10)\n",
    "\n",
    "custom_losses, custom_accuracies, custom_time = custom_nn.train(\n",
    "    X_train, y_train, batch_size=64, epochs=10, learning_rate=0.01)\n",
    "\n",
    "# Evaluate custom model on test data\n",
    "custom_predictions = custom_nn.predict(X_test)\n",
    "custom_test_accuracy = np.mean(custom_predictions == y_test) * 100\n",
    "print(f\"Custom model test accuracy: {custom_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8698256",
   "metadata": {},
   "source": [
    "## 7. Train the PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining PyTorch model...\")\n",
    "pytorch_losses, pytorch_accuracies, pytorch_time = train_pytorch_model(\n",
    "    pytorch_model, train_loader, criterion, py_optimizer, device, num_epochs=10)\n",
    "\n",
    "# Evaluate PyTorch model on test data\n",
    "pytorch_test_accuracy, pytorch_predictions, test_labels = evaluate_pytorch_model(\n",
    "    pytorch_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f055a18",
   "metadata": {},
   "source": [
    "## 8. Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d0d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"{'Model':<20} {'Training Time (s)':<20} {'Test Accuracy (%)':<20}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Custom Neural Net':<20} {custom_time:<20.2f} {custom_test_accuracy:<20.2f}\")\n",
    "print(f\"{'PyTorch Model':<20} {pytorch_time:<20.2f} {pytorch_test_accuracy:<20.2f}\")\n",
    "print(f\"{'Speedup':<20} {custom_time/pytorch_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c536324",
   "metadata": {},
   "source": [
    "## 9. Visualize Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics comparison\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(1, len(custom_losses) + 1), custom_losses, 'b-', label='Custom Implementation')\n",
    "plt.plot(range(1, len(pytorch_losses) + 1), pytorch_losses, 'r-', label='PyTorch Implementation')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(1, len(custom_accuracies) + 1), custom_accuracies, 'b-', label='Custom Implementation')\n",
    "plt.plot(range(1, len(pytorch_accuracies) + 1), pytorch_accuracies, 'r-', label='PyTorch Implementation')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbdad02",
   "metadata": {},
   "source": [
    "## 10. Compare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74017ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions visually\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Select a few random test examples\n",
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(2, num_samples, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f\"True: {y_test[idx]}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "    plt.bar(['Custom', 'PyTorch'], [custom_predictions[idx], pytorch_predictions[idx]], color=['blue', 'red'])\n",
    "    plt.title(f\"Custom: {custom_predictions[idx]}, PyTorch: {pytorch_predictions[idx]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86f81d",
   "metadata": {},
   "source": [
    "## 11. Analyze Prediction Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86893ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples where predictions differ between models\n",
    "different_indices = np.where(custom_predictions != pytorch_predictions)[0]\n",
    "\n",
    "if len(different_indices) > 0:\n",
    "    print(f\"Found {len(different_indices)} examples where models predict differently\")\n",
    "    \n",
    "    # Visualize a few of these examples\n",
    "    num_to_display = min(5, len(different_indices))\n",
    "    display_indices = different_indices[:num_to_display]\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, idx in enumerate(display_indices):\n",
    "        plt.subplot(2, num_to_display, i + 1)\n",
    "        plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"True: {y_test[idx]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, num_to_display, i + 1 + num_to_display)\n",
    "        plt.bar(['Custom', 'PyTorch'], [custom_predictions[idx], pytorch_predictions[idx]], color=['blue', 'red'])\n",
    "        plt.title(f\"Custom: {custom_predictions[idx]}, PyTorch: {pytorch_predictions[idx]}\")\n",
    "    \n",
    "    plt.suptitle(\"Examples with Different Predictions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze which predictions are correct\n",
    "    custom_correct = custom_predictions[different_indices] == y_test[different_indices]\n",
    "    pytorch_correct = pytorch_predictions[different_indices] == y_test[different_indices]\n",
    "    \n",
    "    print(f\"Of the {len(different_indices)} differing predictions:\")\n",
    "    print(f\"  - Custom model is correct in {np.sum(custom_correct)} cases\")\n",
    "    print(f\"  - PyTorch model is correct in {np.sum(pytorch_correct)} cases\")\n",
    "    print(f\"  - Both models are incorrect in {len(different_indices) - np.sum(custom_correct) - np.sum(pytorch_correct)} cases\")\n",
    "else:\n",
    "    print(\"Both models make identical predictions on all test examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62129952",
   "metadata": {},
   "source": [
    "## 12. Discussion: Custom vs. PyTorch Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e35a1",
   "metadata": {},
   "source": [
    "### Key Observations from the Comparison\n",
    "\n",
    "From the experiment above, we can observe several key points:\n",
    "\n",
    "1. **Training Speed**: PyTorch typically trains much faster due to its optimized backend, GPU acceleration, and vectorized operations.\n",
    "\n",
    "2. **Implementation Complexity**: Our custom implementation required explicit coding of the forward and backward passes, while PyTorch handled this automatically through its autograd system.\n",
    "\n",
    "3. **Performance**: Both implementations should theoretically converge to similar results given enough training time, but PyTorch's optimized operations often lead to better performance and stability.\n",
    "\n",
    "4. **Memory Usage**: Custom implementations may use less memory for small models but don't scale as efficiently for larger networks.\n",
    "\n",
    "5. **Code Length**: The custom implementation required significantly more code to achieve the same functionality.\n",
    "\n",
    "### Advantages of Custom Implementation\n",
    "\n",
    "1. **Educational Value**: Building from scratch provides deep understanding of neural network mechanics.\n",
    "2. **Transparency**: Every operation is explicit, making it easier to understand what's happening at each step.\n",
    "3. **Customization**: Complete control over every aspect of the implementation.\n",
    "4. **No Dependencies**: Relies only on NumPy, which is a lightweight dependency.\n",
    "\n",
    "### Advantages of PyTorch Implementation\n",
    "\n",
    "1. **Performance**: Significantly faster training, especially with GPU acceleration.\n",
    "2. **Automatic Differentiation**: No need to manually implement backpropagation.\n",
    "3. **Ecosystem**: Rich set of tools, layers, and pre-built components.\n",
    "4. **Scalability**: Efficiently handles large models and datasets.\n",
    "5. **Production-Ready**: Optimized for real-world applications.\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Use Custom Implementation When:**\n",
    "- Learning the fundamentals of neural networks\n",
    "- Teaching or demonstrating neural network concepts\n",
    "- Implementing a novel algorithm not available in standard libraries\n",
    "- Working in environments with limited dependencies\n",
    "\n",
    "**Use PyTorch (or similar frameworks) When:**\n",
    "- Building practical applications\n",
    "- Working with large datasets\n",
    "- Implementing complex architectures\n",
    "- Focusing on results rather than implementation details\n",
    "- Needing GPU acceleration and performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15a256",
   "metadata": {},
   "source": [
    "## 13. Key Insights from Building Neural Networks from Scratch\n",
    "\n",
    "- **Understanding the Fundamentals**: Implementing neural networks from scratch gives a deep understanding of the core concepts and mathematics behind them.\n",
    "\n",
    "- **Debugging Skills**: Knowledge of the underlying operations makes it easier to diagnose and fix issues in more complex models later.\n",
    "\n",
    "- **Appreciation for Frameworks**: Building from scratch helps you appreciate the convenience and optimizations that frameworks like PyTorch provide.\n",
    "\n",
    "- **Gradient Flow Insights**: Implementing backpropagation manually gives insights into how gradients flow through the network.\n",
    "\n",
    "- **Architectural Understanding**: Deep knowledge of the fundamentals makes it easier to understand and implement new neural network architectures.\n",
    "\n",
    "- **Educational Value**: The exercise of building from scratch is invaluable for education and gaining intuition about how neural networks work.\n",
    "\n",
    "- **Computational Awareness**: Implementing optimizations by hand creates awareness of the computational challenges in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d6f86",
   "metadata": {},
   "source": [
    "## 14. Workshop Conclusion\n",
    "\n",
    "In this workshop, we've:\n",
    "\n",
    "1. **Built Neural Networks from Scratch**: We implemented both a simple and a multi-layer neural network using only NumPy, understanding every step of the process.\n",
    "\n",
    "2. **Explored the MNIST Dataset**: We examined a classic dataset for image classification and prepared it for neural network training.\n",
    "\n",
    "3. **Used PyTorch**: We built neural networks using PyTorch, leveraging its powerful optimizations and automatic differentiation.\n",
    "\n",
    "4. **Compared Implementations**: We conducted a direct comparison of custom and framework-based implementations, understanding the trade-offs.\n",
    "\n",
    "5. **Visualized Results**: We created visualizations to help understand the behavior of our models and their predictions.\n",
    "\n",
    "This workshop served as both an educational exploration of neural network fundamentals and a practical introduction to using modern deep learning frameworks. Whether you continue to build models from scratch or leverage tools like PyTorch, the understanding gained here will be valuable in your future machine learning endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b3bc0",
   "metadata": {},
   "source": [
    "## Next Steps for Further Learning\n",
    "\n",
    "1. **Explore More Complex Architectures**: Try implementing or using CNNs, RNNs, or transformer networks.\n",
    "2. **Experiment with Different Datasets**: Apply your knowledge to other datasets beyond MNIST.\n",
    "3. **Dive Deeper into PyTorch**: Learn more advanced features like custom datasets, data augmentation, and model deployment.\n",
    "4. **Implement Advanced Optimization Techniques**: Explore regularization, batch normalization, and advanced optimization algorithms.\n",
    "5. **Contribute to Open Source**: Apply your understanding by contributing to machine learning libraries or projects."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
